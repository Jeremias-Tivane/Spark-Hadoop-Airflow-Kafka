{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e472942-f4e1-43b0-a96b-cdd36e7325a8",
   "metadata": {},
   "source": [
    "# Getting Started With Spark using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3a782d-9665-4cb9-99b0-4702deaa9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "     |████████████████████████████████| 281.4 MB 13 kB/s              █▊                        | 68.1 MB 63.7 MB/s eta 0:00:04MB 63.7 MB/s eta 0:00:04��████▏                      | 80.4 MB 67.3 MB/s eta 0:00:03MB 67.3 MB/s eta 0:00:03 |████████████████▍               | 144.2 MB 62.3 MB/s eta 0:00:03 | 154.3 MB 62.3 MB/s eta 0:00:03��████████████████████▎         | 195.7 MB 5.7 MB/s eta 0:00:15 \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "     |████████████████████████████████| 198 kB 62.3 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=467d8acf1e9e54b040f65132856aaee9103a713905e514c7665eda26d812ad2d\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1abf027-fb85-4cf1-b286-3d2758ce6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b332a174-ef5b-4d25-8db7-d861ac7c6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e111ff91-5d68-453f-9e7b-d6d690bc8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/13 13:13:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77bda4c7-8ab5-42b5-8fe2-8e344a7c10fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyterlab-djrtivane:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5c63219790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6784c38b-03ae-4c2b-a637-1ea656ccf8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,30)\n",
    "# print first element of iterator\n",
    "print(data[0])\n",
    "len(data)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "xrangeRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4340f87b-f7d7-4c84-94e9-df0f347a4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3e28b3-bfd1-422e-a7cc-c554bb6653c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256ca91f-b34b-463c-af4b-fca8f72c467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt1:  0.9988055229187012\n",
      "dt2:  0.3607344627380371\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "test = sc.parallelize(range(1,50000),4)\n",
    "test.cache()\n",
    "\n",
    "t1 = time.time()\n",
    "# first count will trigger evaluation of count *and* cache\n",
    "count1 = test.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"dt1: \", dt1)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "# second count operates on cached data only\n",
    "count2 = test.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"dt2: \", dt2)\n",
    "\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6976d9-c8b8-445f-a3a5-4af0cf09703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyterlab-djrtivane:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5c63219790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44478eba-6d10-422a-94fa-50110c2f0ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    73  100    73    0     0    246      0 --:--:-- --:--:-- --:--:--   246\n"
     ]
    }
   ],
   "source": [
    "# Download the data first into a local `people.json` file\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb7fe1-d724-43e9-aedb-ffac7df705f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset into a spark dataframe using the `read.json()` function\n",
    "df = spark.read.json(\"people.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75624f38-b38f-45b5-b476-2b4c86f62d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcc31d-c4db-4693-b534-94c5ba974709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cbdac-0772-44c0-9ff4-8c30c0711619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and show basic data columns\n",
    "\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b44f9f-993a-4e1c-a1ca-4e8bcd5ba41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform basic filtering\n",
    "\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c1aeb-db59-413b-9c3a-021b3099cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom basic aggregation of data\n",
    "\n",
    "df.groupBy(\"age\").count().show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff103de-f8c9-48d6-bba1-bd1d6a97c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Exercise - Question 1 - RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07224248-019a-4ff3-8723-347a1b31d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code\n",
    "spark\n",
    "# numbers = range(1, 50)\n",
    "data = range(1,50)\n",
    "\n",
    "# numbers_RDD = ...\n",
    "print(data[0])\n",
    "len(data)\n",
    "\n",
    "# even_numbers_RDD = numbers_RDD.map(lambda x: ..)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# Code block for learners to answer\n",
    "# this will let us know that we created an RDD\n",
    "xrangeRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59300634-f47a-4d5e-829f-89d565338e1e",
   "metadata": {},
   "source": [
    "#### Exercise - Question 2 - DataFrames and SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57845d-8143-4821-917a-832703919a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code\n",
    "spark\n",
    "\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
    "\n",
    "# df = spark.read...\n",
    "df = spark.read.json(\"people2.json\").cache()\n",
    "\n",
    "# df.createTempView..\n",
    "df.createTempView(\"people2\")\n",
    "\n",
    "# spark.sql(\"SELECT ...\")\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daac106-d653-4756-8a17-2b25d2ec811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df[\"age\"]).show()\n",
    "spark.sql(\"Select AVG(age) from people2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7152c3-ae66-4ebe-9b80-d035ca613fae",
   "metadata": {},
   "source": [
    "#### Exercise - Question 3 - SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a548790b-d5df-474d-a654-81934bbda7e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2703/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0c77c-b2af-4260-8324-84342ee29d80",
   "metadata": {},
   "source": [
    "##                                **The End**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a1414-c316-4d6c-85d9-0fbba91762f2",
   "metadata": {},
   "source": [
    "|  Compiled By      |    Last Update    |    Social Media   |\n",
    "| ----------------- | ----------------- | ----------------- |\n",
    "|  Jeremias Tivane  |    12-02-2022     | [Linkedin](https://www.linkedin.com/in/jeremiastivane/) | \n",
    "|  Jeremias Tivane  |    14-02-2022     | [Github](https://github.com/Jeremias-Tivane) | \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a353f4-6e6c-48fb-a4c2-bd2ee02a5957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
